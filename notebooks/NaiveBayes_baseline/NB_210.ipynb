{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code borrowed from w207 # Project 2: Topic Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, you'll work with text data from newsgroup postings on a variety of topics. You'll train classifiers to distinguish between the topics based on the text of the posts. Whereas with digit classification, the input is relatively dense: a 28x28 matrix of pixels, many of which are non-zero, here we'll represent each document with a \"bag-of-words\" model. As you'll see, this makes the feature representation quite sparse -- only a few words of the total vocabulary are active in any given document. The bag-of-words assumption here is that the label depends only on the words; their order is not important.\n",
    "\n",
    "The SK-learn documentation on feature extraction will prove useful:\n",
    "http://scikit-learn.org/stable/modules/feature_extraction.html\n",
    "\n",
    "Each problem can be addressed succinctly with the included packages -- please don't add any more. Grading will be based on writing clean, commented code, along with a few short answers.\n",
    "\n",
    "As always, you're welcome to work on the project in groups and discuss ideas on the course wall, but please prepare your own write-up and write your own code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#preprocessing:\n",
    "# filter out the ones that start with RT retweets\n",
    "# remove empty lines\n",
    "# cat notbot.csv.orig | grep -v \"\\\"RT @\" | grep -v \"\\,RT @\" | grep -v '^[[:space:]]*$' |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\stacia\\Miniconda3\\envs\\python2\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "C:\\Users\\stacia\\Miniconda3\\envs\\python2\\lib\\site-packages\\sklearn\\grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# This tells matplotlib not to try opening a new window for each plot.\n",
    "%matplotlib inline\n",
    "\n",
    "# General libraries.\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# SK-learn libraries for learning.\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "# SK-learn libraries for evaluation.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# SK-learn library for importing the newsgroup data.\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# SK-learn libraries for feature extraction from text.\n",
    "from sklearn.feature_extraction.text import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6400, 1200, 13384, 301)\n",
      "(8316, 395, 6207, 86)\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import string\n",
    "\n",
    "\n",
    "num_total_words = num_bot_words = num_notbot_words = 0\n",
    "num_bot_docs = num_notbot_docs = 0\n",
    "unique_words = {}\n",
    "wordcount = {}\n",
    "bot_test = []\n",
    "bot_train = []\n",
    "notbot_test = []\n",
    "notbot_train= []\n",
    "\n",
    "percent_test = 0.20\n",
    "csvfile = open('bot.csv')\n",
    "readCSV = csv.reader(csvfile)\n",
    "num_bot_docs = sum([1 for row in readCSV])\n",
    "num_bot_train = int(num_bot_docs * (1-percent_test))\n",
    "csvfile = open('notbot.csv')\n",
    "readCSV = csv.reader(csvfile)\n",
    "num_notbot_docs = sum([1 for row in readCSV])\n",
    "num_notbot_train = int(num_notbot_docs * (1-percent_test))\n",
    "\n",
    "############### process bot text ############################33\n",
    "csvfile = open('bot.csv')\n",
    "readCSV = csv.reader(csvfile)\n",
    "ctr = 0\n",
    "\n",
    "for row in readCSV :\n",
    "    if ctr is 0 :\n",
    "        ctr += 1\n",
    "    else :\n",
    "        ctr += 1\n",
    "        if ctr > num_bot_train :\n",
    "            bot_test.append(row[3].lower())\n",
    "        else :\n",
    "            bot_train.append(row[3].lower())\n",
    "            words = [x.strip(string.punctuation) for x in row[3].lower().split()]\n",
    "            num_total_words += len(words)\n",
    "    \n",
    "            for word in words :\n",
    "                if word is not '' :\n",
    "                    num_bot_words += 1\n",
    "                    unique_words[word] = 1\n",
    "                    if word not in wordcount :\n",
    "            \n",
    "                        # first is notbot, 2nd is bot\n",
    "                        wordcount[word] = {0:0, 1:0}\n",
    "                    wordcount[word][1] += 1\n",
    "        \n",
    "csvfile.close()\n",
    "\n",
    "\n",
    "print (len(unique_words), num_bot_train, num_bot_words, len(bot_test))\n",
    "############### process not bot text ############################\n",
    "\n",
    "\n",
    "csvfile = open('notbot.csv')\n",
    "readCSV = csv.reader(csvfile)\n",
    "ctr = 0\n",
    "\n",
    "for row in readCSV :\n",
    "    if ctr is 0 :\n",
    "        ctr += 1\n",
    "    else :\n",
    "        ctr += 1\n",
    "        if len(row) is 11 :\n",
    "          if ctr > num_notbot_train :\n",
    "            notbot_test.append(row[3].lower())\n",
    "          else :\n",
    "            notbot_train.append(row[3].lower())\n",
    "            words = [x.strip(string.punctuation) for x in row[3].lower().split()]\n",
    "            num_total_words += len(words)\n",
    "    \n",
    "            for word in words :\n",
    "                if word is not '' :\n",
    "                    num_notbot_words += 1\n",
    "                    unique_words[word] = 1\n",
    "                    if word not in wordcount :\n",
    "            \n",
    "                         # first is notbot, 2nd is bot\n",
    "                        wordcount[word] = {0:0, 1:0}\n",
    "                    wordcount[word][0] += 1\n",
    "        \n",
    "csvfile.close()\n",
    "\n",
    "print (len(unique_words), num_notbot_train, num_notbot_words, len(notbot_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('model.txt', 'w')\n",
    "\n",
    "# here we calculate the conditional probability P(word|class=notbot) and P(word|class=bot)\n",
    "# P(word|class) = #occurrences of that word in the class / #total number of words in the class\n",
    "for k,v in wordcount.items():\n",
    "    condprob_termnotbot = (v[0] * 1.00000000000000) / num_notbot_words\n",
    "    condprob_termbot = (v[1] * 1.00000000000000) / num_bot_words\n",
    "    f.write(\"%s\\t%d,%d,%0.10f,%0.10f\\n\" % (k,v[0],v[1], condprob_termnotbot, condprob_termbot))\n",
    "\n",
    "#print out the Class Priors\n",
    "notbot_prior = num_notbot_docs * 1.0000000000000 / (num_notbot_docs + num_bot_docs)\n",
    "bot_prior = num_bot_docs * 1.000000000000 / (num_bot_docs + num_notbot_docs)\n",
    "f.write(\"%s\\t%d,%d,%0.10f,%0.10f\\n\" % (\"ClassPriors\", num_notbot_docs, num_bot_docs, notbot_prior, bot_prior))\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import log\n",
    "from math import exp\n",
    "\n",
    "class NaiveBayesModel(object):\n",
    "\n",
    "    def __init__(self, modelFile):\n",
    "        self.model = {}\n",
    "        recordStrs = [s.strip().split('\\n')[0].split('\\t') for s in open(modelFile).readlines()]\n",
    "        for word, statsStr in recordStrs:\n",
    "            nword = re.sub(' ', '', word)\n",
    "            self.model[nword] = map(float, statsStr.split(\",\"))\n",
    "        #Class priors: counts and probs (Pr(Class =0) and Pr(Class =1))\n",
    "        self.c0, self.c1, self.prClass0, self.prClass1 = map(float, self.model[\"ClassPriors\"])\n",
    "\n",
    "        \n",
    "\n",
    "    def classify(self, doc):\n",
    "        # Posterior Probabilities Pr(Class=0| Doc) and Pr(Class=1| Doc) \n",
    "        # Naive Bayes inference Pr(Class=0| Doc)  ~ Pr(Class=0) * Pr(Class=0| word1) * Pr(Class=0| word2)...... \n",
    "        PrClass0GivenDoc = self.prClass0  \n",
    "        PrClass1GivenDoc = self.prClass1\n",
    "        for word in doc:\n",
    "            PrClass0GivenDoc *= self.model[word][2]\n",
    "            PrClass1GivenDoc *= self.model[word][3]\n",
    "        return([PrClass0GivenDoc, PrClass1GivenDoc])\n",
    " \n",
    "    # the natural log based version of this \n",
    "    # helps avoid underflow issues\n",
    "    def classifyInLogs(self, doc):       \n",
    "        # Posterior Probabilities Pr(Class=0| Doc) and Pr(Class=1| Doc) \n",
    "        # Naive Bayes inference Pr(Class=0| Doc)  ~ Pr(Class=0) * Pr(Class=0| word1) * Pr(Class=0| word2)...... \n",
    "        PrClass0GivenDoc = log(self.prClass0)  \n",
    "        PrClass1GivenDoc = log(self.prClass1)\n",
    "        for word in doc:  #NOTE: Improvement: on loading one should convert probs to log probs!\n",
    "            if word in self.model :\n",
    "                c0 = self.model[word][2]\n",
    "                c1 = self.model[word][3]\n",
    "                if c0 != 0:\n",
    "                    PrClass0GivenDoc += log(c0)\n",
    "                else:\n",
    "                    PrClass0GivenDoc = float(\"-inf\")\n",
    "                if c1 != 0:\n",
    "                   PrClass1GivenDoc += log(c1)\n",
    "                else:\n",
    "                    PrClass1GivenDoc = float(\"-inf\")\n",
    "\n",
    "        return([PrClass0GivenDoc, PrClass1GivenDoc])\n",
    "\n",
    "        \n",
    "    def printModel(self):\n",
    "        print \"NaiveBayes Model starts here\\n----------------\"\n",
    "        print \"PRIORS: prClass0=%04.3f, prClass1=%04.3f\" % (self.prClass0, self.prClass1)\n",
    "        for word, stats in self.model.items():\n",
    "            print \"Pr(\",word, \"| Class)\", stats  #Pr(Class=0| Doc)  all stats\n",
    "        print \"NaiveBayes Model ENDS here\\n----------------\"\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(86, 86)\n",
      "(387, 387)\n",
      "('F1 score is : ', 0.84962406015037606)\n",
      "('Accuracy score is : ', 0.79328165374677007)\n",
      "('Recall score is : ', 0.75083056478405319)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "NBModel = NaiveBayesModel(\"model.txt\") \n",
    "f = open('output.txt', 'w')\n",
    "\n",
    "#prediction on test set\n",
    "y_true = []\n",
    "y_pred = []\n",
    "for sent in notbot_test :\n",
    "    words = [x.strip(string.punctuation) for x in sent.split()]\n",
    "    words = [w for w in words if w is not '']\n",
    "    notbot_probability, bot_probability = NBModel.classifyInLogs(words)\n",
    "    #print (notbot_probability, bot_probability)\n",
    "    y_true.append(0)\n",
    "    predicted = 0\n",
    "    if bot_probability > notbot_probability :\n",
    "        predicted = 1\n",
    "        f.write(\"INCORRECT (true=notbot): \" + sent+  \"\\n\")\n",
    "    else :\n",
    "        f.write(\"CORRECT (true=notbot): \" + sent+  \"\\n\")\n",
    "    y_pred.append(predicted)\n",
    "    \n",
    "print (len(y_true), len(y_pred))\n",
    "\n",
    "for sent in bot_test :\n",
    "    words = [x.strip(string.punctuation) for x in sent.split()]\n",
    "    words = [w for w in words if w is not '']\n",
    "    notbot_probability, bot_probability = NBModel.classifyInLogs(words)\n",
    "    y_true.append(1)\n",
    "    predicted = 0\n",
    "    if bot_probability > notbot_probability :\n",
    "        predicted = 1\n",
    "        f.write(\"CORRECT (true=bot):\" + sent+  \"\\n\")\n",
    "    else :\n",
    "        f.write(\"INCORRECT (true=bot):\" + sent +  \"\\n\")\n",
    "    y_pred.append(predicted)\n",
    "    \n",
    "    #print (notbot_probability, bot_probability)\n",
    "print (len(y_true), len(y_pred))\n",
    "print (\"F1 score is : \", metrics.f1_score(y_true, y_pred, average='binary'))\n",
    "print (\"Accuracy score is : \", metrics.accuracy_score(y_true, y_pred))\n",
    "print (\"Recall score is : \", metrics.recall_score(y_true, y_pred))\n",
    "\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#==========================================================================================================="
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
