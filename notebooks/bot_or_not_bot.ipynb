{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bot or Not Bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import plotly\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "plotly.offline.init_notebook_mode(connected=True)\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import *\n",
    "import matplotlib.pyplot as plt\n",
    "import utils\n",
    "import numpy as np\n",
    "import os\n",
    "import importlib\n",
    "from xgboost import XGBClassifier\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = 999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cresci_dir_2017 = '../data/cresci_2017/'\n",
    "cresci_dir_2015 = '../data/cresci_2015/'\n",
    "varol = '../data/varol_users/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "real_users_1 = pd.read_csv(os.path.join(cresci_dir_2017, 'genuine_accounts.csv/users.csv'), index_col=0)\n",
    "real_users_2 = pd.read_csv(os.path.join(cresci_dir_2015, 'E13/users.csv'), index_col=0)\n",
    "real_users_3 = pd.read_csv(os.path.join(cresci_dir_2015, 'TFP/users.csv'), index_col=0)\n",
    "real_users_4 = pd.read_csv(os.path.join(varol, 'varol_real.csv'), index_col=0)\n",
    "\n",
    "# real_tweets = pd.read_csv(os.path.join(cresci_dir_2017, 'genuine_accounts.csv/tweets.csv'), index_col=0, low_memory=False)\n",
    "spam_users_1 = pd.read_csv(os.path.join(cresci_dir_2017, 'social_spambots_1.csv/users.csv'), index_col=0)\n",
    "# spam_tweets_1 = pd.read_csv(os.path.join(cresci_dir_2017, 'social_spambots_1.csv/tweets.csv'), index_col=0, low_memory=False)\n",
    "spam_users_2 = pd.read_csv(os.path.join(cresci_dir_2017, 'social_spambots_2.csv/users.csv'), index_col=0)\n",
    "# spam_tweets_2 = pd.read_csv(os.path.join(cresci_dir_2017, 'social_spambots_2.csv/tweets.csv'), index_col=0, low_memory=False)\n",
    "spam_users_3 = pd.read_csv(os.path.join(cresci_dir_2017, 'social_spambots_3.csv/users.csv'), index_col=0)\n",
    "# spam_tweets_3 = pd.read_csv(os.path.join(cresci_dir_2017, 'social_spambots_3.csv/tweets.csv'), index_col=0, low_memory=False)\n",
    "spam_users_4 = pd.read_csv(os.path.join(cresci_dir_2017, 'traditional_spambots_1.csv/users.csv'), index_col=0)\n",
    "# spam_tweets_4 = pd.read_csv(os.path.join(cresci_dir_2017, 'traditional_spambots_1.csv/tweets.csv'), index_col=0, low_memory=False)\n",
    "spam_users_5 = pd.read_csv(os.path.join(cresci_dir_2017, 'traditional_spambots_2.csv/users.csv'), index_col=0)\n",
    "spam_users_6 = pd.read_csv(os.path.join(cresci_dir_2017, 'traditional_spambots_3.csv/users.csv'), index_col=0)\n",
    "spam_users_7 = pd.read_csv(os.path.join(cresci_dir_2017, 'traditional_spambots_4.csv/users.csv'), index_col=0)\n",
    "spam_users_8 = pd.read_csv(os.path.join(cresci_dir_2015, 'FSF/users.csv'), index_col=0)\n",
    "spam_users_9 = pd.read_csv(os.path.join(cresci_dir_2015, 'INT/users.csv'), index_col=0)\n",
    "spam_users_10 = pd.read_csv(os.path.join(cresci_dir_2015, 'TWT/users.csv'), index_col=0)\n",
    "spam_users_11 = pd.read_csv(os.path.join(varol, 'varol_fake.csv'), index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Leave spam_user_3 for test\n",
    "spam_users_train = pd.concat(objs=[spam_users_1, spam_users_2, spam_users_3, spam_users_4, spam_users_5, spam_users_7,\n",
    "                                  spam_users_8, spam_users_9, spam_users_10, spam_users_11, spam_users_6])\n",
    "# spam_tweets_train = pd.concat(objs=[spam_tweets_2])\n",
    "spam_users_test = spam_users_6\n",
    "# spam_tweets_test = spam_tweets_1\n",
    "\n",
    "# Split geniune user data set to train and test\n",
    "# real_users_train = real_users.loc[real_users_1['test_set_1']==0, :]\n",
    "# real_users_test = real_users.loc[real_users_1['test_set_1']==1, :]\n",
    "\n",
    "# Concatenate cresci-2015 real users data\n",
    "real_users_train = pd.concat(objs=[real_users_1, real_users_3, real_users_2, real_users_4])\n",
    "real_users_test = pd.concat(objs=[real_users_4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "real_users_train.loc[:,'label'] = np.zeros(shape=(real_users_train.shape[0])) \n",
    "real_users_test.loc[:,'label'] = np.zeros(shape=(real_users_test.shape[0]))\n",
    "spam_users_test.loc[:,'label'] = np.ones(shape=(spam_users_test.shape[0]))\n",
    "spam_users_train.loc[:,'label'] = np.ones(shape=(spam_users_train.shape[0]))\n",
    "\n",
    "# concat the genuine users data with the spam data\n",
    "train_set = pd.concat([real_users_train, spam_users_train])\n",
    "test_set = pd.concat([real_users_test, spam_users_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tweet_feats = utils.extract_tweet_feats(all_tweets)\n",
    "\n",
    "# # Merge features from tweets to users\n",
    "# all_users = all_users.merge(tweet_feats, how='inner', left_index=True, right_index=True)\n",
    "\n",
    "reload(utils)\n",
    "# ## Feature selection\n",
    "feat_choices = ['default_profile', 'default_profile_image', 'favourites_count', 'followers_count',\n",
    "               'friends_count', 'geo_enabled', 'listed_count', 'profile_use_background_image',\n",
    "               'statuses_count', 'verified', 'protected', 'profile_background_tile', 'profile_background_color',\n",
    "               'profile_sidebar_border_color', 'profile_sidebar_fill_color']\n",
    "\n",
    "# Training set\n",
    "features, labels, scaler, feat_names = utils.feat_processing(train_set, feat_choices)\n",
    "# Test set \n",
    "test_features, test_labels = utils.feat_processing(test_set, feat_choices, scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pickle.dump(scaler, open('standardscalar.dat', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test with my own twitter account just for fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# temp = pd.read_csv('stacia.csv', index_col=0)\n",
    "# temp['label'] = 0\n",
    "# test_features, test_labels = utils.feat_processing(temp, feat_choices, scaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split dataset to three"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# all_data = pd.concat(objs=[train_set, test_set])\n",
    "# features = all_data.loc[:, all_data.columns != 'labels']\n",
    "# labels = all_data.loc[:,'label']\n",
    "# x_train, x_test, y_train, y_test = train_test_split(features, labels, test_size=0.3, random_state=392)\n",
    "# x_eval, x_test, y_eval, y_test = train_test_split(x_test, y_test, test_size=0.5, random_state=392)\n",
    "# features, labels, scaler, feat_names = utils.feat_processing(x_train, feat_choices)\n",
    "# # Test set \n",
    "# test_features, test_labels = utils.feat_processing(x_test, feat_choices, scaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a gradient boosting classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # gb_classifier = GradientBoostingClassifier()\n",
    "gb_classifier = XGBClassifier()\n",
    "gb_classifier.fit(features, labels)\n",
    "prediction = gb_classifier.predict(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(gb_classifier, open('bot_classifer.dat', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = pickle.load(open('bot_classifer.dat', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importance of each feature:\n",
      "\n",
      "1. friends_count\n",
      "2. favourites_count\n",
      "3. followers_count\n",
      "4. statuses_count\n",
      "5. listed_count\n",
      "6. profile_background_color\n",
      "7. geo_enabled\n",
      "8. profile_use_background_image\n",
      "9. default_profile\n",
      "10. profile_background_tile\n",
      "11. default_profile_image\n",
      "12. real_sidebar_color\n",
      "13. spam_sidebar_color\n",
      "14. profile_sidebar_fill_color\n",
      "15. protected\n",
      "16. verified\n",
      "\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.98      0.95      0.96       681\n",
      "        1.0       0.92      0.96      0.94       403\n",
      "\n",
      "avg / total       0.96      0.95      0.95      1084\n",
      "\n"
     ]
    }
   ],
   "source": [
    "idx = np.argsort(gb_classifier.feature_importances_)[::-1]\n",
    "print 'Importance of each feature:\\n'\n",
    "feat_order = [str(y) + '. ' + str(feat_names[x]) for x,y in zip(idx, range(1, len(feat_names)+1))]\n",
    "\n",
    "for feat in feat_order:\n",
    "    print feat\n",
    "\n",
    "print '\\n'\n",
    "print 'Classification Report:\\n'\n",
    "print classification_report(test_labels, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[647,  34],\n",
       "       [ 15, 388]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(test_labels, prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Grid search cv code\n",
    "# params = {'max_depth':[2, 3, 5], 'min_samples_leaf':[1,2,3,5], \n",
    "#           'min_samples_split':[2,3,5], 'n_estimators':[50,100,200,500]}\n",
    "# clf = GridSearchCV(gb_classifier, params)\n",
    "# clf.fit(x_train, y_train)\n",
    "\n",
    "# Features include tweets\n",
    "# feat_choices = ['default_profile', 'default_profile_image', 'favourites_count', 'followers_count',\n",
    "#                'friends_count', 'geo_enabled', 'listed_count', 'profile_use_background_image',\n",
    "#                'statuses_count', 'verified', 'tweet_len', 'contain_RT', 'num_mentions', 'num_hashtags', \n",
    "#                'num_urls', 'retweet_count']\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
