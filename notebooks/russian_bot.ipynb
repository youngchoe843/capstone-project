{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import *\n",
    "import utils\n",
    "pd.options.display.max_columns = 999\n",
    "pd.options.display.max_colwidth = 800\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from imblearn.over_sampling import SMOTE, ADASYN, RandomOverSampler\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "import plotly\n",
    "plotly.offline.init_notebook_mode(connected=True)\n",
    "from keras.models import Sequential, optimizers\n",
    "from keras.layers import Dense\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM\n",
    "import importlib\n",
    "import utils\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/tweets/tweets.csv', low_memory=False, header=0, encoding = \"ISO-8859-1\")\n",
    "data['BotType']=data['BotType'].apply(lambda x:0 if x== 'Traditional' else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "importlib.reload(utils)\n",
    "data.loc[:,'text'] = utils.clean_tweets(data.loc[:,'text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mitigate class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Undersample traditional bots\n",
    "a = data[data['BotType']==0].sample(frac=.2)\n",
    "b = data[data['BotType']==1]\n",
    "data  = pd.concat([a,b],axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>BotType</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>124909</th>\n",
       "      <td>@steehjns vi e fiquei babando, morri kkkkkkk podia ter de verdade, impresso ): ia fazer sucesso!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101652</th>\n",
       "      <td>seguindo \\u2014 - http://4ms.me/bKWf23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4907</th>\n",
       "      <td>New post, \"Delta to Sell Piceance Properties\" - http://bit.ly/8Zg50o</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6890</th>\n",
       "      <td>Emerging-Market Shares Advance for 4th Day on Earnings, Greece - BusinessWeek http://tinyurl.com/2wn895j</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38810</th>\n",
       "      <td>@amanda_jg *---* oii floor :D</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                            text  \\\n",
       "124909          @steehjns vi e fiquei babando, morri kkkkkkk podia ter de verdade, impresso ): ia fazer sucesso!   \n",
       "101652                                                                    seguindo \\u2014 - http://4ms.me/bKWf23   \n",
       "4907                                        New post, \"Delta to Sell Piceance Properties\" - http://bit.ly/8Zg50o   \n",
       "6890    Emerging-Market Shares Advance for 4th Day on Earnings, Greece - BusinessWeek http://tinyurl.com/2wn895j   \n",
       "38810                                                                              @amanda_jg *---* oii floor :D   \n",
       "\n",
       "        BotType  \n",
       "124909        0  \n",
       "101652        0  \n",
       "4907          0  \n",
       "6890          0  \n",
       "38810         0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# x_train, x_test, y_train, y_test = train_test_split(data.loc[:,'text'], data['BotType'], test_size=0.2, random_state=321)\n",
    "x_train = data.loc[:,'text']\n",
    "y_train = data.loc[:,'BotType']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf-idf conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(stop_words='english', token_pattern='[\\w]+', ngram_range=(1,2))\n",
    "x_train = tfidf.fit_transform(x_train)\n",
    "# x_test = tfidf.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf_choice = 'mlp'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc1b58f5576e41f6b259e7e62dbe4e6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac3a88a220af4a9a81a3c8cddb6eca3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e9fddfabd384524bc83affb52a564f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e428c523a7d45608b73c0ee1abdb583",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c45c624ddad4eaf95a602331a6d98ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "183ac09a2dff41cb8d278d19923a4555",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking : expected dense_1_input to have shape (198299,) but got array with shape (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-3a1e620e8134>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTQDMNotebookCallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# predict the test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32melif\u001b[0m \u001b[0mclf_choice\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'lstm'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# LSTM implementation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mpredict_classes\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1136\u001b[0m         \"\"\"\n\u001b[1;32m   1137\u001b[0m         proba = self.predict(x, batch_size=batch_size, verbose=verbose,\n\u001b[0;32m-> 1138\u001b[0;31m                              steps=steps)\n\u001b[0m\u001b[1;32m   1139\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mproba\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1140\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mproba\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1023\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m         return self.model.predict(x, batch_size=batch_size, verbose=verbose,\n\u001b[0;32m-> 1025\u001b[0;31m                                   steps=steps)\n\u001b[0m\u001b[1;32m   1026\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1815\u001b[0m         x = _standardize_input_data(x, self._feed_input_names,\n\u001b[1;32m   1816\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1817\u001b[0;31m                                     check_batch_axis=False)\n\u001b[0m\u001b[1;32m   1818\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1819\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    121\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    124\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking : expected dense_1_input to have shape (198299,) but got array with shape (1,)"
     ]
    }
   ],
   "source": [
    "if clf_choice == 'xgb':\n",
    "    # xgboost classifier\n",
    "    clf = XGBClassifier(n_jobs=4)\n",
    "    clf.fit(x_train, y_train)\n",
    "    y_pred = clf.predict(x_test)\n",
    "elif clf_choice == 'mlp':\n",
    "    # Simply multilayer perceptron\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(100, input_dim=x_train.shape[1], activation='relu'))\n",
    "#     model.add(Dense(20, activation='relu'))\n",
    "    model.add(Dense(8, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    # Fit the model\n",
    "    model.fit(x_train, y_train, epochs=5, batch_size=20, verbose=0, callbacks=[TQDMNotebookCallback()])\n",
    "    # predict the test set\n",
    "#     y_pred = model.predict_classes(x_test)\n",
    "elif clf_choice == 'lstm':\n",
    "    # LSTM implementation\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=x_train.shape[1], output_dim=32))\n",
    "    model.add(LSTM(16, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    opt_params = optimizers.RMSprop(lr=0.01)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt_params, metrics=['accuracy'])\n",
    "    model.fit(x_train, y_train, epochs=2, batch_size=100, verbose=0, callbacks=[TQDMNotebookCallback()])\n",
    "#     y_pred = model.predict_classes(x_test)\n",
    "    \n",
    "    \n",
    "# print(classification_report(y_test, y_pred))\n",
    "# confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "keras.models.Sequential"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't pickle _thread.lock objects",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-bd4b0bc59122>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfidf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tfidf.dat'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mlp.dat'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: can't pickle _thread.lock objects"
     ]
    }
   ],
   "source": [
    "pickle.dump(tfidf, open('tfidf.dat', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model.save('mlp.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = load_model('mlp.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       ..., \n",
       "       [1],\n",
       "       [1],\n",
       "       [1]], dtype=int32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.predict_classes(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "type": "scatter",
         "x": [
          "http",
          "twitter com",
          "politics",
          "û_",
          "jugendmitmerkel",
          "news",
          "_",
          "sports",
          "o",
          "police",
          "http bit",
          "bit",
          "columbianchemicals",
          "bit ly",
          "crime",
          "ly",
          "rt",
          "trump",
          "com",
          "û",
          "ûªt",
          "www",
          "__",
          "seu",
          "cleveland",
          "e",
          "u0e19",
          "don",
          "chicago",
          "t",
          "ä",
          "n",
          "obama",
          "u0e23",
          "news http",
          "ûªs",
          "gets",
          "___",
          "nova",
          "http www",
          "que",
          "cek123",
          "pic",
          "don t",
          "m",
          "people",
          "xe3o",
          "â",
          "â_",
          "s",
          "eu ttbm",
          "eu tnm",
          "eu vc",
          "eu va",
          "eu tive",
          "eu v",
          "eu tiveer",
          "eu tivesse",
          "eu utilizo",
          "eu uso",
          "eu tou",
          "eu tt",
          "eu usei",
          "eu uma",
          "eu um",
          "eu trabalhando",
          "euro breaks",
          "eu tire",
          "eu u2014",
          "eu troco",
          "eu u03b1ndo",
          "eu tiro",
          "eurico bivar",
          "eu tinha",
          "eu sei",
          "eu siim",
          "eu sigo",
          "eu ser",
          "eu senti",
          "eu sempre",
          "eu seja",
          "euro der",
          "euro buy",
          "eu seguindo",
          "euro dollar",
          "eu salvo",
          "eu saio",
          "eu saindo",
          "eu sabia",
          "eu sinto",
          "eu sonhando",
          "eu ti",
          "eu tbm",
          "eu testei",
          "eu termino",
          "eu ter",
          "eu tenho",
          "eu teamo",
          "eu te",
          "eu tb"
         ],
         "y": [
          0.14957265555858612,
          0.08760683983564377,
          0.07051282376050949,
          0.06410256773233414,
          0.0555555559694767,
          0.044871795922517776,
          0.03632478788495064,
          0.03418803587555885,
          0.02777777798473835,
          0.025641025975346565,
          0.025641025975346565,
          0.02350427396595478,
          0.021367521956562996,
          0.021367521956562996,
          0.01923076994717121,
          0.01923076994717121,
          0.017094017937779427,
          0.017094017937779427,
          0.017094017937779427,
          0.017094017937779427,
          0.014957264997065067,
          0.014957264997065067,
          0.014957264997065067,
          0.012820512987673283,
          0.012820512987673283,
          0.010683760978281498,
          0.008547008968889713,
          0.008547008968889713,
          0.008547008968889713,
          0.008547008968889713,
          0.008547008968889713,
          0.006410256493836641,
          0.006410256493836641,
          0.006410256493836641,
          0.006410256493836641,
          0.006410256493836641,
          0.004273504484444857,
          0.004273504484444857,
          0.004273504484444857,
          0.004273504484444857,
          0.004273504484444857,
          0.004273504484444857,
          0.004273504484444857,
          0.004273504484444857,
          0.004273504484444857,
          0.0021367522422224283,
          0.0021367522422224283,
          0.0021367522422224283,
          0.0021367522422224283,
          0.0021367522422224283
         ]
        }
       ],
       "layout": {
        "title": "Feature Importance",
        "xaxis": {
         "title": "Top 100 Words"
        },
        "yaxis": {
         "title": "Feature Weights"
        }
       }
      },
      "text/html": [
       "<div id=\"17caa68f-20ea-4123-b500-1258a2440660\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"17caa68f-20ea-4123-b500-1258a2440660\", [{\"type\": \"scatter\", \"x\": [\"http\", \"twitter com\", \"politics\", \"\\u00fb_\", \"jugendmitmerkel\", \"news\", \"_\", \"sports\", \"o\", \"police\", \"http bit\", \"bit\", \"columbianchemicals\", \"bit ly\", \"crime\", \"ly\", \"rt\", \"trump\", \"com\", \"\\u00fb\", \"\\u00fb\\u00aat\", \"www\", \"__\", \"seu\", \"cleveland\", \"e\", \"u0e19\", \"don\", \"chicago\", \"t\", \"\\u00e4\", \"n\", \"obama\", \"u0e23\", \"news http\", \"\\u00fb\\u00aas\", \"gets\", \"___\", \"nova\", \"http www\", \"que\", \"cek123\", \"pic\", \"don t\", \"m\", \"people\", \"xe3o\", \"\\u00e2\", \"\\u00e2_\", \"s\", \"eu ttbm\", \"eu tnm\", \"eu vc\", \"eu va\", \"eu tive\", \"eu v\", \"eu tiveer\", \"eu tivesse\", \"eu utilizo\", \"eu uso\", \"eu tou\", \"eu tt\", \"eu usei\", \"eu uma\", \"eu um\", \"eu trabalhando\", \"euro breaks\", \"eu tire\", \"eu u2014\", \"eu troco\", \"eu u03b1ndo\", \"eu tiro\", \"eurico bivar\", \"eu tinha\", \"eu sei\", \"eu siim\", \"eu sigo\", \"eu ser\", \"eu senti\", \"eu sempre\", \"eu seja\", \"euro der\", \"euro buy\", \"eu seguindo\", \"euro dollar\", \"eu salvo\", \"eu saio\", \"eu saindo\", \"eu sabia\", \"eu sinto\", \"eu sonhando\", \"eu ti\", \"eu tbm\", \"eu testei\", \"eu termino\", \"eu ter\", \"eu tenho\", \"eu teamo\", \"eu te\", \"eu tb\"], \"y\": [0.14957265555858612, 0.08760683983564377, 0.07051282376050949, 0.06410256773233414, 0.0555555559694767, 0.044871795922517776, 0.03632478788495064, 0.03418803587555885, 0.02777777798473835, 0.025641025975346565, 0.025641025975346565, 0.02350427396595478, 0.021367521956562996, 0.021367521956562996, 0.01923076994717121, 0.01923076994717121, 0.017094017937779427, 0.017094017937779427, 0.017094017937779427, 0.017094017937779427, 0.014957264997065067, 0.014957264997065067, 0.014957264997065067, 0.012820512987673283, 0.012820512987673283, 0.010683760978281498, 0.008547008968889713, 0.008547008968889713, 0.008547008968889713, 0.008547008968889713, 0.008547008968889713, 0.006410256493836641, 0.006410256493836641, 0.006410256493836641, 0.006410256493836641, 0.006410256493836641, 0.004273504484444857, 0.004273504484444857, 0.004273504484444857, 0.004273504484444857, 0.004273504484444857, 0.004273504484444857, 0.004273504484444857, 0.004273504484444857, 0.004273504484444857, 0.0021367522422224283, 0.0021367522422224283, 0.0021367522422224283, 0.0021367522422224283, 0.0021367522422224283]}], {\"title\": \"Feature Importance\", \"xaxis\": {\"title\": \"Top 100 Words\"}, \"yaxis\": {\"title\": \"Feature Weights\"}}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"17caa68f-20ea-4123-b500-1258a2440660\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"17caa68f-20ea-4123-b500-1258a2440660\", [{\"type\": \"scatter\", \"x\": [\"http\", \"twitter com\", \"politics\", \"\\u00fb_\", \"jugendmitmerkel\", \"news\", \"_\", \"sports\", \"o\", \"police\", \"http bit\", \"bit\", \"columbianchemicals\", \"bit ly\", \"crime\", \"ly\", \"rt\", \"trump\", \"com\", \"\\u00fb\", \"\\u00fb\\u00aat\", \"www\", \"__\", \"seu\", \"cleveland\", \"e\", \"u0e19\", \"don\", \"chicago\", \"t\", \"\\u00e4\", \"n\", \"obama\", \"u0e23\", \"news http\", \"\\u00fb\\u00aas\", \"gets\", \"___\", \"nova\", \"http www\", \"que\", \"cek123\", \"pic\", \"don t\", \"m\", \"people\", \"xe3o\", \"\\u00e2\", \"\\u00e2_\", \"s\", \"eu ttbm\", \"eu tnm\", \"eu vc\", \"eu va\", \"eu tive\", \"eu v\", \"eu tiveer\", \"eu tivesse\", \"eu utilizo\", \"eu uso\", \"eu tou\", \"eu tt\", \"eu usei\", \"eu uma\", \"eu um\", \"eu trabalhando\", \"euro breaks\", \"eu tire\", \"eu u2014\", \"eu troco\", \"eu u03b1ndo\", \"eu tiro\", \"eurico bivar\", \"eu tinha\", \"eu sei\", \"eu siim\", \"eu sigo\", \"eu ser\", \"eu senti\", \"eu sempre\", \"eu seja\", \"euro der\", \"euro buy\", \"eu seguindo\", \"euro dollar\", \"eu salvo\", \"eu saio\", \"eu saindo\", \"eu sabia\", \"eu sinto\", \"eu sonhando\", \"eu ti\", \"eu tbm\", \"eu testei\", \"eu termino\", \"eu ter\", \"eu tenho\", \"eu teamo\", \"eu te\", \"eu tb\"], \"y\": [0.14957265555858612, 0.08760683983564377, 0.07051282376050949, 0.06410256773233414, 0.0555555559694767, 0.044871795922517776, 0.03632478788495064, 0.03418803587555885, 0.02777777798473835, 0.025641025975346565, 0.025641025975346565, 0.02350427396595478, 0.021367521956562996, 0.021367521956562996, 0.01923076994717121, 0.01923076994717121, 0.017094017937779427, 0.017094017937779427, 0.017094017937779427, 0.017094017937779427, 0.014957264997065067, 0.014957264997065067, 0.014957264997065067, 0.012820512987673283, 0.012820512987673283, 0.010683760978281498, 0.008547008968889713, 0.008547008968889713, 0.008547008968889713, 0.008547008968889713, 0.008547008968889713, 0.006410256493836641, 0.006410256493836641, 0.006410256493836641, 0.006410256493836641, 0.006410256493836641, 0.004273504484444857, 0.004273504484444857, 0.004273504484444857, 0.004273504484444857, 0.004273504484444857, 0.004273504484444857, 0.004273504484444857, 0.004273504484444857, 0.004273504484444857, 0.0021367522422224283, 0.0021367522422224283, 0.0021367522422224283, 0.0021367522422224283, 0.0021367522422224283]}], {\"title\": \"Feature Importance\", \"xaxis\": {\"title\": \"Top 100 Words\"}, \"yaxis\": {\"title\": \"Feature Weights\"}}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if clf_choice == 'xgb':\n",
    "    idx = np.argsort(clf.feature_importances_)[::-1]\n",
    "    most_freq_words = [tfidf.get_feature_names()[x] for x in idx[:100]]\n",
    "    trace = go.Scatter(x= most_freq_words, y=np.sort(clf.feature_importances_)[::-1][:50])\n",
    "    layout = dict(title='Feature Importance', xaxis=dict(title='Top 100 Words'), yaxis=dict(title='Feature Weights'))\n",
    "    fig = dict(data=[trace], layout=layout)\n",
    "    plotly.offline.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# clf.predict_proba(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code that does word2vec, did not work well..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model = gensim.models.Word2Vec(x_train, workers=4, size=300)\n",
    "# w2v = dict(zip(model.wv.index2word, model.wv.vectors))\n",
    "# temp = MeanEmbeddingVectorizer(w2v)\n",
    "# temp.fit(x_train)\n",
    "# x_train = temp.transform(x_train)\n",
    "# x_test = temp.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oversampling technique, did not do as well as undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# x_train, y_train = RandomOverSampler().fit_sample(x_train, y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
