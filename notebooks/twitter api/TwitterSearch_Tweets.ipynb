{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter API Call\n",
    "\n",
    "## w210 Capstone\n",
    "## Spring 2018\n",
    "\n",
    "By: Vyas, Phat, Stacia, Kasey\n",
    "\n",
    "Last update: 01/24/2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Twitter live streaming API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import twittersearch API\n",
    "from TwitterSearch import * ## please `pip install TwitterSearch` if not yet installed \n",
    "#import TwitterSearch\n",
    "import pandas as pd\n",
    "import json ## to load json format into dict\n",
    "import time ## to extract date/time from retrieved data\n",
    "#import datetime ## to get current date/time## Authentication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twitter authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## twitter authentication\n",
    "ts = TwitterSearch(\n",
    "        consumer_key = 'X2SwXG1wIBLJiJRUunqIXyaNz',\n",
    "        consumer_secret = 'j0mtZ5BvcNE26rdRrySprP3iYhiAglUn8qtzGP5Rw4WXqm1lV8',\n",
    "        access_token = '41741727-9JoDDAAoTUo47z8Zo8uCQ1t5WLcpWnoQVSrImoB7q',\n",
    "        access_token_secret = 'dhEDyLezeg22qClu1qrIf3pVadN3n9KOxK8q72bCwvG42'\n",
    "        \n",
    "#         consumer_key = \"Vk9kCWPUw8KctdN4QGywZFpvd\",\n",
    "#         consumer_secret =  \"CoPsToBCXDYWkaxoSnPf9qnNEPazxvHGDzexDzZ6KJxq9ZplQb\",\n",
    "#         access_token =  \"820596649-FSomVy8OakTPYrCKrJQw2IfsUZXGk5Xp6kCzfbFt\",\n",
    "#         access_token_secret =  \"c2jLc2HNQPKFGncaeNQzRonB4B9LOOKOU67lVcaOeEzYK\"\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_time_transform(date_time):\n",
    "    date_time_deconstruct = time.strptime(date_time, \"%a %b %d %H:%M:%S +0000 %Y\")\n",
    "    date_output = str(date_time_deconstruct.tm_year) + '/' + str(date_time_deconstruct.tm_mon) + '/' + str(date_time_deconstruct.tm_mday)\n",
    "    time_output = str(date_time_deconstruct.tm_hour) + ':' + str(date_time_deconstruct.tm_min)\n",
    "    return date_output, time_output\n",
    "\n",
    "def date_time_transform2(date_time):\n",
    "    date_time_deconstruct = time.strptime(date_time, \"%a %b %d %H:%M:%S %Y\")\n",
    "    date_output = str(date_time_deconstruct.tm_year) + str(date_time_deconstruct.tm_mon) + str(date_time_deconstruct.tm_mday)\n",
    "    time_output = str(date_time_deconstruct.tm_hour) + str(date_time_deconstruct.tm_min)\n",
    "    return date_output, time_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search by keyword w/ wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_keyword(keyword_list):\n",
    "    \"\"\"\n",
    "        arg: list of keywords. This list is used as AND for all keywords input, therefore limit to only necessary pairs\n",
    "        return: list of tweets\n",
    "    \"\"\"\n",
    "    try:\n",
    "        tso = TwitterSearchOrder() # create a TwitterSearchOrder object\n",
    "        tso.set_keywords(keyword_list) ## list of keywords. \n",
    "        tso.set_language('en') ## English tweets only\n",
    "        tso.set_include_entities(False) ## and don't give us all those entity information ## and don't give us all those entity information\n",
    "        tso.set_result_type('mixed') ## Include both popular and real time results in the response.\n",
    "\n",
    "        ## to avoid rate-limitation\n",
    "        sleep_for = 10 # sleep for 60 seconds\n",
    "        last_amount_of_queries = 0 # used to detect when new queries are done\n",
    "\n",
    "        list_output = [] ## store retrieved tweets and their metadata in a list object\n",
    "        count = 0\n",
    "        for tweet in ts.search_tweets_iterable(tso):\n",
    "            ## get the first tweet and store all metadata for reference later\n",
    "#             if count < 1:\n",
    "#                 full_sample = [] \n",
    "#                 for key in tweet:\n",
    "#                     full_sample.append([key, tweet[key]])\n",
    "\n",
    "#             ## print 5 tweets to indicate sucessful loading\n",
    "#             if count < 5:\n",
    "#                 print( '@%s tweeted: %s | on %s' % ( tweet['user']['screen_name'], tweet['text'], tweet['created_at'] ) )\n",
    "#                 print('---------------------------------------')\n",
    "#                 count += 1\n",
    "\n",
    "            ## extract and split date time into its individual varaiable\n",
    "            date_output, time_output = date_time_transform(tweet['created_at'])        \n",
    "\n",
    "            ## extract relevant information from the tweet metadata\n",
    "            output = [\n",
    "                #['created_at', tweet['created_at']] ## date/time of tweet\n",
    "                ['date', date_output]\n",
    "                ,['time', time_output]\n",
    "                ,['id_str', tweet['id_str']] ## user id\n",
    "                ,['text',tweet['text']] ## actual tweet\n",
    "                ,['user_friends_count', tweet['user']['friends_count']]\n",
    "                ,['user_followers_count', tweet['user']['followers_count']]\n",
    "                ,['user_location', tweet['user']['location']]\n",
    "                ,['user_screen_name', tweet['user']['screen_name']]\n",
    "                ,['user_verified', tweet['user']['verified']] ## if account is a verified account\n",
    "                ,['coordinates', tweet['coordinates']]\n",
    "                ,['retweet_count', tweet['retweet_count']]\n",
    "                ]\n",
    "            list_output.append(output)\n",
    "\n",
    "            ## to avoid rate-limitation\n",
    "            current_amount_of_queries = ts.get_statistics()[0]\n",
    "            if not last_amount_of_queries == current_amount_of_queries:\n",
    "                last_amount_of_queries = current_amount_of_queries\n",
    "                time.sleep(sleep_for)\n",
    "\n",
    "        return list_output\n",
    "\n",
    "    except TwitterSearchException as e: # take care of all those ugly errors if there are some\n",
    "        print(e)\n",
    "        return None\n",
    "\n",
    "def search_keyword_wrapper(keywords):\n",
    "    \"\"\"\n",
    "        arg: list of keywords and use search_keyword() function to retrieve tweets associated with each keyword\n",
    "        return: export all retrieved tweets into a dataframe and save in csv format\n",
    "    \"\"\"\n",
    "    \n",
    "    tempt_list = [] ## temporary list for extracting tweets\n",
    "    \n",
    "    ## loop through the list of keywords and retrieve tweets related to each keyword\n",
    "    for keyword in keywords:\n",
    "        print(\"-----------------------------------\")\n",
    "        print(\"Keyword(s):\",keyword)\n",
    "        results = search_keyword(keyword)\n",
    "        print(\"Numbers of tweets:\",len(results))\n",
    "        tempt_list.append(results)\n",
    "\n",
    "    output = []\n",
    "    for l in range(len(tempt_list)):\n",
    "        for i in range(len(tempt_list[l])):\n",
    "            tempt = []\n",
    "            for j in range(len(tempt_list[l][i])):\n",
    "                tempt.append(tempt_list[l][i][j][1])\n",
    "            output.append(tempt)\n",
    "\n",
    "    ## create columns names\n",
    "    fields =  ['date', 'time', 'id_str', 'text', 'user_friends_count', 'user_followers_count', 'user_location'\n",
    "               ,'user_screen_name', 'user_verified','coordinates', 'retweet_count']\n",
    "\n",
    "    ## transform temp_list into data frame for exportation\n",
    "    df_output = pd.DataFrame(data = output, columns = fields)\n",
    "\n",
    "    ## get date/time for \n",
    "    date_retrieved, time_retrieved = date_time_transform2(time.ctime())\n",
    "    file_name = '../data/tweets/search_keyword_' + str(date_retrieved) + '_' + str(time_retrieved) + '.csv'\n",
    "\n",
    "    df_output.to_csv(file_name,  sep = ',', index = False)\n",
    "\n",
    "    print('Successfully export file ', file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "Keyword(s): ['price']\n",
      "Numbers of tweets: 1998\n",
      "-----------------------------------\n",
      "Keyword(s): ['lend']\n",
      "Numbers of tweets: 1697\n",
      "-----------------------------------\n",
      "Keyword(s): ['chew']\n",
      "Numbers of tweets: 1999\n",
      "-----------------------------------\n",
      "Keyword(s): ['emphasis']\n",
      "Numbers of tweets: 24499\n",
      "-----------------------------------\n",
      "Keyword(s): ['retirement']\n",
      "Numbers of tweets: 14799\n",
      "-----------------------------------\n",
      "Keyword(s): ['system']\n",
      "Numbers of tweets: 11292\n",
      "-----------------------------------\n",
      "Keyword(s): ['Sunday']\n",
      "Numbers of tweets: 1694\n",
      "-----------------------------------\n",
      "Keyword(s): ['chase']\n",
      "Numbers of tweets: 55394\n",
      "-----------------------------------\n",
      "Keyword(s): ['guest']\n",
      "Numbers of tweets: 2596\n",
      "-----------------------------------\n",
      "Keyword(s): ['define']\n",
      "Numbers of tweets: 11196\n",
      "Successfully export file  ../data/tweets/search_keyword_2018128_1932.csv\n"
     ]
    }
   ],
   "source": [
    "keyword_list = [['price']\n",
    "                ,['lend']\n",
    "                ,['chew']\n",
    "                ,['emphasis']\n",
    "                ,['retirement']\n",
    "                ,['system']\n",
    "                ,['Sunday']\n",
    "                ,['chase']\n",
    "                ,['guest']\n",
    "                ,['define']\n",
    "               ]\n",
    "\n",
    "search_keyword_wrapper(keyword_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_list = [['#GRAMMYs']\n",
    "                ,['#ProBowl']\n",
    "                ,['#NHLAllStarGame']\n",
    "                ,['#RoyalRumble']\n",
    "                ,['#TwitterCourseTopics']\n",
    "                ,['Jay-Z']\n",
    "                ,['Harrison Smith']\n",
    "                ,['Marquette']\n",
    "                ,['IKEA']\n",
    "                ,['Isaac Haas']\n",
    "               ]\n",
    "\n",
    "search_keyword_wrapper(keyword_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_list = [['trump']\n",
    "            ,['clinton']\n",
    "            ,['russia','trump']\n",
    "            ,['election']\n",
    "            ,['republican']\n",
    "            ,['democrat']\n",
    "            ,['#maga']\n",
    "           ]\n",
    "\n",
    "search_keyword_wrapper(keyword_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## print out the full metada list for reference\n",
    "#full_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search by username"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_user(username):\n",
    "    \"\"\"\n",
    "        arg: twitter username\n",
    "        return: list of tweets\n",
    "    \"\"\"\n",
    "    try:\n",
    "        tuo = TwitterUserOrder(username) # create a TwitterUserOrder\n",
    "        \n",
    "        ## to avoid rate-limitation\n",
    "#         sleep_for = 10 # sleep for 60 seconds\n",
    "#         last_amount_of_queries = 0 # used to detect when new queries are done\n",
    "\n",
    "        # start asking Twitter about the timeline\n",
    "        list_output = [] ## store retrieved tweets and their metadata in a list object\n",
    "#         count = 0\n",
    "        for tweet in ts.search_tweets_iterable(tuo):\n",
    "#             ## get the first tweet and store all metadata for reference later\n",
    "#             if count < 1:\n",
    "#                 full_sample = [] \n",
    "#                 for key in tweet:\n",
    "#                     full_sample.append([key, tweet[key]])\n",
    "\n",
    "#             ## print 5 tweets to indicate sucessful loading\n",
    "#             if count < 5:\n",
    "#                 print( '@%s tweeted: %s | on %s' % ( tweet['user']['screen_name'], tweet['text'], tweet['created_at'] ) )\n",
    "#                 print('---------------------------------------')\n",
    "#                 count += 1\n",
    "\n",
    "            ## extract and split date time into its individual varaiable\n",
    "            date_output, time_output = date_time_transform(tweet['created_at'])        \n",
    "\n",
    "            ## extract relevant information from the tweet metadata\n",
    "            output = [\n",
    "                #['created_at', tweet['created_at']] ## date/time of tweet\n",
    "                ['date', date_output]\n",
    "                ,['time', time_output]\n",
    "                ,['id_str', tweet['id_str']] ## user id\n",
    "                ,['text',tweet['text']] ## actual tweet\n",
    "                ,['user_friends_count', tweet['user']['friends_count']]\n",
    "                ,['user_followers_count', tweet['user']['followers_count']]\n",
    "                ,['user_location', tweet['user']['location']]\n",
    "                ,['user_screen_name', tweet['user']['screen_name']]\n",
    "                ,['user_verified', tweet['user']['verified']] ## if account is a verified account\n",
    "                ,['coordinates', tweet['coordinates']]\n",
    "                ,['retweet_count', tweet['retweet_count']]\n",
    "                ]\n",
    "            list_output.append(output)\n",
    "            \n",
    "            ## to avoid rate-limitation\n",
    "#             current_amount_of_queries = ts.get_statistics()[0]\n",
    "#             if not last_amount_of_queries == current_amount_of_queries:\n",
    "#                 last_amount_of_queries = current_amount_of_queries\n",
    "#                 time.sleep(sleep_for)\n",
    "        \n",
    "        return list_output\n",
    "\n",
    "    except TwitterSearchException as e: # take care of all those ugly errors if there are some\n",
    "        print(e)\n",
    "        return None\n",
    "    \n",
    "def search_user_wrapper(user):\n",
    "    user_results = search_user(user)\n",
    "    len(user_results)\n",
    "    \n",
    "    output = []\n",
    "    for i in range(len(user_results)):\n",
    "        tempt = []\n",
    "        for j in range(len(user_results[i])):\n",
    "            tempt.append(user_results[i][j][1])\n",
    "        output.append(tempt)\n",
    "\n",
    "    ## create columns names\n",
    "    fields =  ['date', 'time', 'id_str', 'text', 'user_friends_count', 'user_followers_count', 'user_location'\n",
    "               ,'user_screen_name', 'user_verified','coordinates', 'retweet_count']\n",
    "\n",
    "    ## transform temp_list into data frame for exportation\n",
    "    df_output = pd.DataFrame(data = output, columns = fields)\n",
    "\n",
    "    ## get date/time for \n",
    "    date_retrieved, time_retrieved = date_time_transform2(time.ctime())\n",
    "    file_name = '../data/tweets/search_user_' + user + '_' + str(date_retrieved) + '_' + str(time_retrieved) + '.csv'\n",
    "\n",
    "    df_output.to_csv(file_name,  sep = ',', index = False)\n",
    "\n",
    "    print('Successfully export file ', file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3202"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user = 'washingtonpost'\n",
    "search_user_wrapper(user)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search by UserID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_userid(userid):\n",
    "    \"\"\"\n",
    "        arg: twitter userid\n",
    "        return: list of tweets\n",
    "    \"\"\"\n",
    "    try:\n",
    "        tuo = TwitterUserOrder(userid) # create a TwitterUserOrder\n",
    "        \n",
    "        ## to avoid rate-limitation\n",
    "        sleep_for = 10 # sleep for 60 seconds\n",
    "        last_amount_of_queries = 0 # used to detect when new queries are done\n",
    "\n",
    "        # start asking Twitter about the timeline\n",
    "        list_output = [] ## store retrieved tweets and their metadata in a list object\n",
    "#         count = 0\n",
    "        for tweet in ts.search_tweets_iterable(tuo):\n",
    "#             ## get the first tweet and store all metadata for reference later\n",
    "#             if count < 1:\n",
    "#                 full_sample = [] \n",
    "#                 for key in tweet:\n",
    "#                     full_sample.append([key, tweet[key]])\n",
    "\n",
    "#             ## print 5 tweets to indicate sucessful loading\n",
    "#             if count < 5:\n",
    "#                 print( '@%s tweeted: %s | on %s' % ( tweet['user']['screen_name'], tweet['text'], tweet['created_at'] ) )\n",
    "#                 print('---------------------------------------')\n",
    "#                 count += 1\n",
    "\n",
    "            ## extract and split date time into its individual varaiable\n",
    "            date_output, time_output = date_time_transform(tweet['created_at'])        \n",
    "\n",
    "            ## extract relevant information from the tweet metadata\n",
    "            output = [\n",
    "                #['created_at', tweet['created_at']] ## date/time of tweet\n",
    "                ['date', date_output]\n",
    "                ,['time', time_output]\n",
    "                ,['id_str', tweet['id_str']] ## user id\n",
    "                ,['text',tweet['text']] ## actual tweet\n",
    "                ,['user_friends_count', tweet['user']['friends_count']]\n",
    "                ,['user_followers_count', tweet['user']['followers_count']]\n",
    "                ,['user_location', tweet['user']['location']]\n",
    "                ,['user_screen_name', tweet['user']['screen_name']]\n",
    "                ,['user_verified', tweet['user']['verified']] ## if account is a verified account\n",
    "                ,['coordinates', tweet['coordinates']]\n",
    "                ,['retweet_count', tweet['retweet_count']]\n",
    "                ]\n",
    "            list_output.append(output)\n",
    "            \n",
    "            ## to avoid rate-limitation\n",
    "            current_amount_of_queries = ts.get_statistics()[0]\n",
    "            if not last_amount_of_queries == current_amount_of_queries:\n",
    "                last_amount_of_queries = current_amount_of_queries\n",
    "                time.sleep(sleep_for)\n",
    "        \n",
    "        return list_output\n",
    "\n",
    "    except TwitterSearchException as e: # take care of all those ugly errors if there are some\n",
    "        print(e)\n",
    "        return None\n",
    "    \n",
    "def search_userid_wrapper(user):\n",
    "    user_results = search_userid(user)\n",
    "    print(\"User id:\",str(user))\n",
    "    len(user_results)\n",
    "    \n",
    "    output = []\n",
    "    for i in range(len(user_results)):\n",
    "        tempt = []\n",
    "        for j in range(len(user_results[i])):\n",
    "            tempt.append(user_results[i][j][1])\n",
    "        output.append(tempt)\n",
    "\n",
    "    ## create columns names\n",
    "    fields =  ['date', 'time', 'id_str', 'text', 'user_friends_count', 'user_followers_count', 'user_location'\n",
    "               ,'user_screen_name', 'user_verified','coordinates', 'retweet_count']\n",
    "\n",
    "    ## transform temp_list into data frame for exportation\n",
    "    df_output = pd.DataFrame(data = output, columns = fields)\n",
    "\n",
    "    ## get date/time for \n",
    "    date_retrieved, time_retrieved = date_time_transform2(time.ctime())\n",
    "    file_name = '../data/tweets/search_userid_' + str(user) + '_' + str(date_retrieved) + '_' + str(time_retrieved) + '.csv'\n",
    "\n",
    "    df_output.to_csv(file_name,  sep = ',', index = False)\n",
    "\n",
    "    print('Successfully export file ', file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User id: 3098421349\n",
      "Successfully export file  ../data/tweets/search_userid_3098421349_201822_2123.csv\n"
     ]
    }
   ],
   "source": [
    "user = 3098421349\n",
    "search_userid_wrapper(user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    tso = TwitterSearchOrder() # create a TwitterSearchOrder object\n",
    "    tso.set_keywords(['trump'#,'russia','clinton','america','american'\n",
    "                      #,'Trump','Russia','Clinton','America','American'\n",
    "                      #,'election','democrats','republican'\n",
    "                      #,'#Trump','#Russia','#Clinton','#American'\n",
    "                     ]) ## list of keywords. This list is used as AND\n",
    "    tso.set_language('en') ## English tweets only\n",
    "    tso.set_include_entities(False) ## and don't give us all those entity information ## and don't give us all those entity information\n",
    "    tso.set_result_type('mixed') ## Include both popular and real time results in the response.\n",
    "\n",
    "    list_output = [] ## store retrieved tweets and their metadata in a list object\n",
    "    count = 0\n",
    "    for tweet in ts.search_tweets_iterable(tso):\n",
    "        ## get the first tweet and store all metadata for reference later\n",
    "        if count < 1:\n",
    "            full_sample = [] \n",
    "            for key in tweet:\n",
    "                full_sample.append([key, tweet[key]])\n",
    "\n",
    "        ## print 5 tweets to indicate sucessful loading\n",
    "        if count < 5:\n",
    "            print( '@%s tweeted: %s | on %s' % ( tweet['user']['screen_name'], tweet['text'], tweet['created_at'] ) )\n",
    "            print('---------------------------------------')\n",
    "            count += 1\n",
    "\n",
    "        ## extract and split date time into its individual varaiable\n",
    "        date_output, time_output = date_time_transform(tweet['created_at'])        \n",
    "\n",
    "        ## extract relevant information from the tweet metadata\n",
    "        output = [\n",
    "            #['created_at', tweet['created_at']] ## date/time of tweet\n",
    "            ['date', date_output]\n",
    "            ,['time', time_output]\n",
    "            ,['id_str', tweet['id_str']] ## user id\n",
    "            ,['text',tweet['text']] ## actual tweet\n",
    "            ,['user_friends_count', tweet['user']['friends_count']]\n",
    "            ,['user_location', tweet['user']['location']]\n",
    "            ,['user_screen_name', tweet['user']['screen_name']]\n",
    "            ,['user_verified', tweet['user']['verified']] ## if account is a verified account\n",
    "            ,['coordinates', tweet['coordinates']]\n",
    "            ,['retweet_count', tweet['retweet_count']]\n",
    "        ]\n",
    "\n",
    "        list_output.append(output)\n",
    "\n",
    "except TwitterSearchException as e: # take care of all those ugly errors if there are some\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## query to retrieve twwet\n",
    "def search_keyword(keyword_list):\n",
    "    \"\"\"\n",
    "        arg: list of keywords. This list is used as AND for all keywords input, therefore limit to only necessary pairs\n",
    "        return: list of tweets\n",
    "    \"\"\"\n",
    "    try:\n",
    "        tso = TwitterSearchOrder() # create a TwitterSearchOrder object\n",
    "        tso.set_keywords(keyword_list) ## list of keywords. \n",
    "        tso.set_language('en') ## English tweets only\n",
    "        tso.set_include_entities(False) ## and don't give us all those entity information ## and don't give us all those entity information\n",
    "        tso.set_result_type('mixed') ## Include both popular and real time results in the response.\n",
    "\n",
    "        ## to avoid rate-limitation\n",
    "        sleep_for = 10 # sleep for 60 seconds\n",
    "        last_amount_of_queries = 0 # used to detect when new queries are done\n",
    "\n",
    "        list_output = [] ## store retrieved tweets and their metadata in a list object\n",
    "        count = 0\n",
    "        for tweet in ts.search_tweets_iterable(tso):\n",
    "            ## get the first tweet and store all metadata for reference later\n",
    "#             if count < 1:\n",
    "#                 full_sample = [] \n",
    "#                 for key in tweet:\n",
    "#                     full_sample.append([key, tweet[key]])\n",
    "\n",
    "#             ## print 5 tweets to indicate sucessful loading\n",
    "#             if count < 5:\n",
    "#                 print( '@%s tweeted: %s | on %s' % ( tweet['user']['screen_name'], tweet['text'], tweet['created_at'] ) )\n",
    "#                 print('---------------------------------------')\n",
    "#                 count += 1\n",
    "\n",
    "            ## extract and split date time into its individual varaiable\n",
    "            date_output, time_output = date_time_transform(tweet['created_at'])        \n",
    "\n",
    "            ## extract relevant information from the tweet metadata\n",
    "            output = [\n",
    "                #['created_at', tweet['created_at']] ## date/time of tweet\n",
    "                ['date', date_output]\n",
    "                ,['time', time_output]\n",
    "                ,['id_str', tweet['id_str']] ## user id\n",
    "                ,['text',tweet['text']] ## actual tweet\n",
    "                ,['user_friends_count', tweet['user']['friends_count']]\n",
    "                ,['user_followers_count', tweet['user']['followers_count']]\n",
    "                ,['user_location', tweet['user']['location']]\n",
    "                ,['user_screen_name', tweet['user']['screen_name']]\n",
    "                ,['user_verified', tweet['user']['verified']] ## if account is a verified account\n",
    "                ,['coordinates', tweet['coordinates']]\n",
    "                ,['retweet_count', tweet['retweet_count']]\n",
    "                ]\n",
    "            list_output.append(output)\n",
    "            \n",
    "            ## to avoid rate-limitation\n",
    "            current_amount_of_queries = ts.get_statistics()[0]\n",
    "            if not last_amount_of_queries == current_amount_of_queries:\n",
    "                last_amount_of_queries = current_amount_of_queries\n",
    "                time.sleep(sleep_for)\n",
    "\n",
    "        return list_output\n",
    "\n",
    "    except TwitterSearchException as e: # take care of all those ugly errors if there are some\n",
    "        print(e)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "Keyword(s): ['trump']\n",
      "Numbers of tweets: 699\n",
      "-----------------------------------\n",
      "Keyword(s): ['clinton']\n",
      "Numbers of tweets: 228899\n",
      "-----------------------------------\n",
      "Keyword(s): ['#maga']\n",
      "Numbers of tweets: 96\n",
      "699\n",
      "[['date', '2018/1/25'], ['time', '21:14'], ['id_str', '956636374228176900'], ['text', 'What about the 19 women who accused Trump of sexual assault and harassment? Are you inspired by their stories?  https://t.co/LmbpWWzaEn'], ['user_friends_count', 1489], ['user_followers_count', 20536], ['user_location', 'Brooklyn, NY'], ['user_screen_name', 'LEBassett'], ['user_verified', True], ['coordinates', None], ['retweet_count', 9158]]\n"
     ]
    }
   ],
   "source": [
    "keywords = [['trump']\n",
    "            ,['clinton']\n",
    "#             ,['russia','trump']\n",
    "#             ,['election']\n",
    "#             ,['republican']\n",
    "#             ,['democrat']\n",
    "            ,['#maga']\n",
    "           ]\n",
    "\n",
    "tempt_list = [] ## temporary list for extracting data\n",
    "for keyword in keywords:\n",
    "    print(\"-----------------------------------\")\n",
    "    print(\"Keyword(s):\",keyword)\n",
    "    results = search_keyword(keyword)\n",
    "    print(\"Numbers of tweets:\",len(results))\n",
    "    tempt_list.append(results)\n",
    "\n",
    "## Check if it's successful\n",
    "print(len(tempt_list[0]))\n",
    "print(tempt_list[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully export file  ../data/search_keyword_2018127_158.csv\n"
     ]
    }
   ],
   "source": [
    "output = []\n",
    "for l in range(len(tempt_list)):\n",
    "    for i in range(len(tempt_list[l])):\n",
    "        tempt = []\n",
    "        for j in range(len(tempt_list[l][i])):\n",
    "            tempt.append(tempt_list[l][i][j][1])\n",
    "        output.append(tempt)\n",
    "\n",
    "## create columns names\n",
    "fields =  ['date', 'time', 'id_str', 'text', 'user_friends_count', 'user_location', 'user_followers_count'\n",
    "           ,'user_screen_name', 'user_verified','coordinates', 'retweet_count']\n",
    "\n",
    "## transform temp_list into data frame for exportation\n",
    "df_output = pd.DataFrame(data = output, columns = fields)\n",
    "\n",
    "## get date/time for \n",
    "date_retrieved, time_retrieved = date_time_transform2(time.ctime())\n",
    "file_name = '../data/search_keyword_' + str(date_retrieved) + '_' + str(time_retrieved) + '.csv'\n",
    "\n",
    "df_output.to_csv(file_name,  sep = ',', index = False)\n",
    "\n",
    "print('Successfully export file ', file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "Keyword(s): ['russian', 'investigation']\n",
      "Numbers of tweets: 1190\n",
      "-----------------------------------\n",
      "Keyword(s): ['election']\n",
      "Numbers of tweets: 27999\n",
      "-----------------------------------\n",
      "Keyword(s): ['republican']\n",
      "Numbers of tweets: 79089\n",
      "-----------------------------------\n",
      "Keyword(s): ['democrat']\n",
      "Numbers of tweets: 13289\n",
      "1190\n",
      "[['date', '2018/1/25'], ['time', '20:38'], ['id_str', '956627401357787137'], ['text', \"For years, Dutch intelligence had access to Russia's Cozy Bear hackers group. Crucial evidence for Russian interfer… https://t.co/ENj8n2AzEa\"], ['user_friends_count', 937], ['user_followers_count', 13010], ['user_location', '#standplaatslaptop'], ['user_screen_name', 'trbrtc'], ['user_verified', True], ['coordinates', None], ['retweet_count', 1616]]\n"
     ]
    }
   ],
   "source": [
    "keywords = [\n",
    "             ['russian','investigation']    \n",
    "            ,['election']\n",
    "            ,['republican']\n",
    "            ,['democrat']\n",
    "           ]\n",
    "\n",
    "tempt_list = [] ## temporary list for extracting data\n",
    "for keyword in keywords:\n",
    "    print(\"-----------------------------------\")\n",
    "    print(\"Keyword(s):\",keyword)\n",
    "    results = search_keyword(keyword)\n",
    "    print(\"Numbers of tweets:\",len(results))\n",
    "    tempt_list.append(results)\n",
    "\n",
    "## Check if it's successful\n",
    "print(len(tempt_list[0]))\n",
    "print(tempt_list[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully export file  ../data/search_keyword_2018127_533.csv\n"
     ]
    }
   ],
   "source": [
    "output = []\n",
    "for l in range(len(tempt_list)):\n",
    "    for i in range(len(tempt_list[l])):\n",
    "        tempt = []\n",
    "        for j in range(len(tempt_list[l][i])):\n",
    "            tempt.append(tempt_list[l][i][j][1])\n",
    "        output.append(tempt)\n",
    "\n",
    "## create columns names\n",
    "fields =  ['date', 'time', 'id_str', 'text', 'user_friends_count', 'user_location', 'user_followers_count'\n",
    "           ,'user_screen_name', 'user_verified','coordinates', 'retweet_count']\n",
    "\n",
    "## transform temp_list into data frame for exportation\n",
    "df_output = pd.DataFrame(data = output, columns = fields)\n",
    "\n",
    "## get date/time for \n",
    "date_retrieved, time_retrieved = date_time_transform2(time.ctime())\n",
    "file_name = '../data/search_keyword_' + str(date_retrieved) + '_' + str(time_retrieved) + '.csv'\n",
    "\n",
    "df_output.to_csv(file_name,  sep = ',', index = False)\n",
    "\n",
    "print('Successfully export file ', file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['date', '2018/1/26'],\n",
       " ['time', '1:25'],\n",
       " ['id_str', '956699594091630593'],\n",
       " ['text',\n",
       "  'RT @samstein: hahahaha. everyone on the Hill who has been like, Trump won’t try to fire mueller, he knows that would be a red line. https:/…'],\n",
       " ['user_friends_count', 741],\n",
       " ['user_location', 'Miami, FL'],\n",
       " ['user_screen_name', 'HDBtweets'],\n",
       " ['user_verified', False],\n",
       " ['coordinates', None],\n",
       " ['retweet_count', 74]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## print out the first retrieved tweet and its metadata\n",
    "results[5]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
